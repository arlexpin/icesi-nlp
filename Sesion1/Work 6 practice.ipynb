{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd4590b",
   "metadata": {},
   "source": [
    "# Análisis de Texto con spaCy\n",
    "## Práctica: Búsqueda de Patrones en Percy Jackson\n",
    "\n",
    "Este notebook demuestra técnicas de Procesamiento de Lenguaje Natural (NLP) usando spaCy para:\n",
    "- Descargar y procesar texto de libros\n",
    "- Tokenizar y analizar documentos\n",
    "- Buscar patrones específicos usando Matcher\n",
    "- Extraer contexto alrededor de coincidencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869ece7",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial\n",
    "\n",
    "Verificamos si estamos en Google Colab para descargar dependencias si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1252182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31df26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !wget https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt\n",
    "    %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29e3f5",
   "metadata": {},
   "source": [
    "## 2. Cargar spaCy\n",
    "\n",
    "Cargamos el modelo de inglés `en_core_web_sm` que incluye:\n",
    "- Tokenizador\n",
    "- Part-of-Speech (POS) tagger\n",
    "- Dependency parser\n",
    "- Named Entity Recognizer (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd574cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c178ebe",
   "metadata": {},
   "source": [
    "## 3. Descargar Dataset\n",
    "\n",
    "Descargamos los libros de Percy Jackson desde Kaggle y copiamos los archivos a la carpeta actual para facilitar su acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a145ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (1.0.0).\n",
      "Path to dataset files: C:\\Users\\apapa\\.cache\\kagglehub\\datasets\\shobhit043\\percy-jackson-first-5-books\\versions\\1\n",
      "Files downloaded: ['percy_jackson_book_1.txt', 'percy_jackson_book_2.txt', 'percy_jackson_book_3.txt', 'percy_jackson_book_4.txt', 'percy_jackson_book_5.txt']\n",
      "Files in current directory: ['1-spacy-basics.ipynb', '2-tokenization.ipynb', '3-stemming.ipynb', '4-lemmatization.ipynb', '5-vocabulary.ipynb', '6-practice.ipynb', '7-sentiment-analysis.ipynb', 'moviereviews.tsv', 'owlcreek.txt', 'percy_jackson_book_1.txt', 'percy_jackson_book_2.txt', 'percy_jackson_book_3.txt', 'percy_jackson_book_4.txt', 'percy_jackson_book_5.txt', 'reaganomics.txt', 'sentimentdataset.csv', 'Work 6 practice.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shobhit043/percy-jackson-first-5-books\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Files downloaded:\", os.listdir(path))\n",
    "\n",
    "# Copy files to current directory\n",
    "for file in os.listdir(path):\n",
    "    src = os.path.join(path, file)\n",
    "    dst = os.path.join(\".\", file)\n",
    "    if os.path.isfile(src):\n",
    "        shutil.copy(src, dst)\n",
    "    elif os.path.isdir(src):\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "\n",
    "print(\"Files in current directory:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fcf1b",
   "metadata": {},
   "source": [
    "## 4. Procesar el Documento\n",
    "\n",
    "Leemos el primer libro de Percy Jackson y lo procesamos con spaCy. El objeto `doc` contiene:\n",
    "- Tokens individuales\n",
    "- Información gramatical (POS tags, dependencias)\n",
    "- Entidades nombradas\n",
    "- Estructura de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef51d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento procesado: 127898 tokens\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo y procesarlo con spaCy\n",
    "with open('./percy_jackson_book_1.txt', encoding='utf-8') as file:\n",
    "    doc = nlp(file.read())\n",
    "\n",
    "print(f\"Documento procesado: {len(doc)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a593f",
   "metadata": {},
   "source": [
    "### 4.1 Vista Previa del Documento\n",
    "\n",
    "Mostramos los primeros 50 tokens para verificar que el documento se cargó correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c5cebd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "BOOKS BY RICK RIORDAN\n",
       "\n",
       "PERCY JACKSON AND THE OLYMPIANS\n",
       "The Lightning Thief\n",
       "The Sea of Monsters\n",
       "The Titan’s Curse\n",
       "The Battle of the Labyrinth\n",
       "The Last Olympian\n",
       "The Demigod Files\n",
       "Percy Jackson’s Greek Gods, illustrated by John Rocco"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeros 50 tokens del documento\n",
    "doc[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75032c9",
   "metadata": {},
   "source": [
    "## 5. Búsqueda de Patrones con Matcher\n",
    "\n",
    "Usamos `Matcher` de spaCy para encontrar la frase \"peanut butter\" en el documento.\n",
    "\n",
    "**¿Cómo funciona?**\n",
    "- Definimos un patrón: dos tokens consecutivos con texto \"peanut\" y \"butter\" (en minúsculas)\n",
    "- El Matcher busca todas las ocurrencias en el documento\n",
    "- Devuelve tuplas `(match_id, start, end)` con la posición de cada coincidencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bee2965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18194338103975822726, 1326, 1328), (18194338103975822726, 1551, 1553)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LOWER': 'peanut'}, {'LOWER': 'butter'}]\n",
    "matcher.add(\"like\", [pattern])\n",
    "\n",
    "found_matches = matcher(doc)\n",
    "found_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136cc0f",
   "metadata": {},
   "source": [
    "### 5.1 Contexto de las Coincidencias\n",
    "\n",
    "Veamos el contexto alrededor de cada coincidencia (tokens antes y después) para entender mejor el uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a4d3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the back of the\n",
       "\n",
       "\n",
       "head with chunks of peanut butter-and-ketchup sandwich.\n",
       "Grover was an easy target."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = found_matches[0][1:]\n",
    "doc[start-9:end+13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75edf561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "“It’s okay. I like peanut butter.”\n",
       "He dodged"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = found_matches[1][1:]\n",
    "doc[start-7:end+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b263c",
   "metadata": {},
   "source": [
    "### 5.2 Oraciones Completas\n",
    "\n",
    "Extraemos las oraciones completas que contienen las coincidencias. Esto proporciona el contexto completo y gramaticalmente correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc51c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oraciones que contienen 'peanut butter':\n",
      "\n",
      "1. All the way into the city, I put up with Nancy Bobofit, the freckly,\n",
      "redheaded kleptomaniac girl, hitting my best friend Grover in the back of the\n",
      "\n",
      "\fhead with chunks of peanut butter-and-ketchup sandwich.\n",
      "\n",
      "2. I like peanut butter.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Primero obtenemos todas las oraciones del documento\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print(f\"Oraciones que contienen 'peanut butter':\\n\")\n",
    "\n",
    "# Buscamos cada coincidencia en las oraciones\n",
    "for i, (_, start, end) in enumerate(found_matches, 1):\n",
    "    for sentence in sentences:\n",
    "        # Verificar si la coincidencia está dentro de esta oración\n",
    "        if sentence.start <= start and sentence.end >= end:\n",
    "            print(f\"{i}. {sentence.text.strip()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
